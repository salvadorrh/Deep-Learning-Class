{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mez4QPC8ROrc"
      },
      "source": [
        "# **Implementing Attention from Scratch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp_0YEf6RjRy"
      },
      "source": [
        "Paper: https://arxiv.org/abs/1706.03762\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uLwqPcfRmW6"
      },
      "source": [
        "## Step 1: Divide input into tokens and extract embeddings from each token.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lCRA0ZsSllh"
      },
      "source": [
        "For simplicity, let's assume you have one example represented by 4 tokens, each token represented by an embedding of size 3. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rE8itz-Q9Bs",
        "outputId": "c32249a6-a9fa-4d30-dcc0-0c5ca56d1160"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.96702984, 0.54723225, 0.97268436],\n",
              "       [0.71481599, 0.69772882, 0.2160895 ],\n",
              "       [0.97627445, 0.00623026, 0.25298236],\n",
              "       [0.43479153, 0.77938292, 0.19768507]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.random.seed(4)\n",
        "\n",
        "n_tokens = 4\n",
        "emb_dim = 3\n",
        "\n",
        "emb_rep = np.random.uniform(size=(n_tokens,emb_dim))\n",
        "emb_rep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWAfTN0qV2wA"
      },
      "source": [
        "## Step 2: Create positional embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkjjE_9NWF2f"
      },
      "source": [
        "Text from the paper: \"Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
        "order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add 'positional encodings' to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension $d_{model}$ - emb_dim in our code - as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed.\"\n",
        "\n",
        "This is what they used in the paper: \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZv0ZCCJZBPn"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmVj-pc9ZF2g"
      },
      "source": [
        "where *pos* is the position of the token in the sequence (in our case, since we have 4 tokens, pos ranges from 0 to 3), and *i* represents an embdedding dimension (in our case, since we use embeddings of size 3, i ranges from 0 to 2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ywvon5yZDXq",
        "outputId": "3c471e08-352e-4dad-ff6e-b1ac927d23b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         1.         0.        ]\n",
            " [0.84147098 0.99892298 0.00215443]\n",
            " [0.90929743 0.99569422 0.00430886]\n",
            " [0.14112001 0.9903207  0.00646326]]\n"
          ]
        }
      ],
      "source": [
        "pos = np.arange(n_tokens)\n",
        "i = np.arange(emb_dim)\n",
        "\n",
        "sinusoid_arg = 1/(10000**(i/emb_dim))\n",
        "\n",
        "pos_t = pos.reshape((-1, 1))\n",
        "sinusoid_arg = sinusoid_arg.reshape((1,-1))\n",
        "\n",
        "pos_emd = pos_t @ sinusoid_arg\n",
        "\n",
        "pos_emd[:,::2] = np.sin(pos_emd[:,::2])\n",
        "pos_emd[:,1::2] = np.cos(pos_emd[:,1::2])\n",
        "\n",
        "print(pos_emd)\n",
        "\n",
        "# sinusoid_arg = pos.T @ sinusoid_arg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHhVODIWGh7d"
      },
      "source": [
        "## Step 3: Add input and positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kMFsB3rV6Kl",
        "outputId": "e80b69e0-e52e-4f7e-a2e2-c43ced0eb59f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emb_rep\n",
            " [[0.96702984 0.54723225 0.97268436]\n",
            " [0.71481599 0.69772882 0.2160895 ]\n",
            " [0.97627445 0.00623026 0.25298236]\n",
            " [0.43479153 0.77938292 0.19768507]]\n",
            "pos_emd\n",
            " [[0.         1.         0.        ]\n",
            " [0.84147098 0.99892298 0.00215443]\n",
            " [0.90929743 0.99569422 0.00430886]\n",
            " [0.14112001 0.9903207  0.00646326]]\n",
            "emb_rep\n",
            " [[0.96702984 1.54723225 0.97268436]\n",
            " [1.55628698 1.6966518  0.21824393]\n",
            " [1.88557188 1.00192448 0.25729122]\n",
            " [0.57591154 1.76970362 0.20414833]]\n",
            "emb_rep.shape\n",
            " (4, 3)\n"
          ]
        }
      ],
      "source": [
        "print(\"emb_rep\\n\", emb_rep)\n",
        "print(\"pos_emd\\n\", pos_emd)\n",
        "emb_rep = emb_rep + pos_emd\n",
        "print(\"emb_rep\\n\", emb_rep)\n",
        "print(\"emb_rep.shape\\n\", emb_rep.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Je39vRHtGk"
      },
      "source": [
        "## Step 4: Compute Query, Key, Value matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9d6nkj_HzsV",
        "outputId": "8badb2ae-f910-4a00-aa63-3921a01c7b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q.shape:  (4, 5)\n",
            "k.shape:  (4, 5)\n",
            "v.shape:  (4, 7)\n"
          ]
        }
      ],
      "source": [
        "dim_qk = 5\n",
        "dim_v = 7\n",
        "\n",
        "W_q = np.random.uniform(size=(emb_dim, dim_qk))\n",
        "W_k = np.random.uniform(size=(emb_dim, dim_qk))\n",
        "W_v = np.random.uniform(size=(emb_dim, dim_v))\n",
        "\n",
        "q = emb_rep @ W_q\n",
        "k = emb_rep @ W_k\n",
        "v = emb_rep @ W_v\n",
        "\n",
        "print(\"q.shape: \", q.shape)\n",
        "print(\"k.shape: \", k.shape)\n",
        "print(\"v.shape: \", v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8tV50m3Im4J"
      },
      "source": [
        "## Step 5: Create attention scores using Q and K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hA_dR6IIsZD",
        "outputId": "454a39e1-7f82-4d68-fba5-2b26ca3cd0a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention_scores.shape:  (4, 4)\n",
            "attention_scores\n",
            " [[0.37965353 0.32430831 0.25857327 0.03746489]\n",
            " [0.38588555 0.32354765 0.25136182 0.03920498]\n",
            " [0.37262363 0.32077205 0.25548236 0.05112196]\n",
            " [0.36111936 0.31497959 0.26234658 0.06155447]]\n"
          ]
        }
      ],
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "attention_scores = softmax((q @ k.T) / np.sqrt(dim_qk), axis=1)\n",
        "\n",
        "print(\"attention_scores.shape: \", attention_scores.shape)\n",
        "print(\"attention_scores\\n\", attention_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1B6oXlqMm69"
      },
      "source": [
        "## Step 6: Compute attention head output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Qi1_y-_Vpff",
        "outputId": "d909ce86-59e1-4a52-eb38-5586852f5953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention_head_out.shape:  (4, 7)\n",
            "attention_head_out\n",
            " [[2.44633317 1.77935922 1.47644343 1.84107525 1.69358435 1.28678952\n",
            "  1.20331499]\n",
            " [2.44704884 1.78363618 1.47870694 1.84169269 1.69304248 1.28869591\n",
            "  1.20123723]\n",
            " [2.43591964 1.77885368 1.46944765 1.83625089 1.68225793 1.28400026\n",
            "  1.19652617]\n",
            " [2.42571811 1.77289667 1.46106617 1.83062819 1.67326142 1.27909535\n",
            "  1.19261518]]\n"
          ]
        }
      ],
      "source": [
        "attention_head_out = attention_scores @ v\n",
        "print(\"attention_head_out.shape: \", attention_head_out.shape)\n",
        "print(\"attention_head_out\\n\", attention_head_out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwa3o_81y9mV"
      },
      "source": [
        "For multi-head attention, concatenate the outputs of all heads and multiply the resulting matrix times a final W_out weight matrix.\n",
        "\n",
        "In practice, it's common for *key* and *value* to be the same tensor. \n",
        "\n",
        "    dim_qk = 5\n",
        "\n",
        "    W_q = np.random.uniform(size=(emb_dim, dim_qk))\n",
        "    W_kv = np.random.uniform(size=(emb_dim, dim_qk))\n",
        "\n",
        "    q = emb_rep @ W_q\n",
        "    v = emb_rep @ W_kv\n",
        "    k = v\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI1pmm9f41m9"
      },
      "source": [
        "## Let's try to implement this stuff using Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvH6cUDB5PSa"
      },
      "source": [
        "Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vUSrCGWy5Qj9"
      },
      "outputs": [],
      "source": [
        "# Import the libraries we'll use below.\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns  # for nicer plots\n",
        "sns.set(style='darkgrid')  # default style\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWsb_15T470e"
      },
      "source": [
        "Load some data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcjOlFG15aFY",
        "outputId": "32b97ad9-e486-4e90-ce8d-bb666e2e6ac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "X_train.shape (60000, 28, 28)\n",
            "X_test.shape (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "# Load the Fashion MNIST dataset.\n",
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "# Flatten Y_train and Y_test, so they become vectors of label values.\n",
        "# The label for X_train[0] is in Y_train[0].\n",
        "Y_train = Y_train.flatten()\n",
        "Y_test = Y_test.flatten()\n",
        "\n",
        "np.random.seed(0) # For reproducibility purposes\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Shuffle the order of the training examples.\n",
        "indices = np.arange(X_train.shape[0])\n",
        "shuffled_indices = np.random.permutation(indices)\n",
        "\n",
        "X_train = X_train[shuffled_indices]\n",
        "Y_train = Y_train[shuffled_indices]\n",
        "\n",
        "print(\"X_train.shape\", X_train.shape)\n",
        "print(\"X_test.shape\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao8pvH9m9BZ1",
        "outputId": "9b297c90-d1f3-4b5b-cb62-9805cd706ada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "X_train.shape (60000, 16, 49)\n",
            "X_test.shape (10000, 16, 49)\n"
          ]
        }
      ],
      "source": [
        "# Let's represent each image as a sequence of feature vectors\n",
        "n_patches = 16\n",
        "n_patch_pixels = (X_train.shape[1] * X_train.shape[2]) // n_patches\n",
        "\n",
        "# TODO: Get actual patches\n",
        "X_train = X_train.reshape((X_train.shape[0], -1))\n",
        "X_test = X_test.reshape((X_test.shape[0], -1))\n",
        "X_train = X_train.reshape((X_train.shape[0], n_patches, n_patch_pixels))\n",
        "X_test = X_test.reshape((X_test.shape[0], n_patches, n_patch_pixels))\n",
        "\n",
        "print()\n",
        "print(\"X_train.shape\", X_train.shape)\n",
        "print(\"X_test.shape\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyto2OS79Fw1"
      },
      "source": [
        "Let's code the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SGM9rRVa9I0n"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_model(input_shape, n_classes):\n",
        "  tf.keras.backend.clear_session()\n",
        "  # Create the model\n",
        "  emb_dim = 32\n",
        "  query_key_dim = 32\n",
        "  value_dim = 32\n",
        "\n",
        "  input_layer = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "  # Extract embeddings of size emb_dim from each patch\n",
        "  emb_extractor = layers.Dense(emb_dim, activation=\"relu\")\n",
        "  embeddings = layers.TimeDistributed(emb_extractor)(input_layer)\n",
        "  # embeddings = input_layer\n",
        "\n",
        "  query = layers.Dense(query_key_dim)(embeddings)\n",
        "  key = layers.Dense(query_key_dim)(embeddings)\n",
        "  value = layers.Dense(value_dim)(embeddings)\n",
        "\n",
        "  attention_scores = tf.linalg.matmul(query, key, transpose_b=True)\n",
        "  attention_scores = attention_scores / tf.math.sqrt(float(query_key_dim))\n",
        "\n",
        "  attention_scores = tf.nn.softmax(attention_scores, axis=-1)\n",
        "\n",
        "  attention_head_out = tf.linalg.matmul(attention_scores, value)\n",
        "\n",
        "  x = layers.Flatten()(attention_head_out)\n",
        "  x = layers.Dense(64, activation='relu')(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  x = layers.Dense(32, activation='relu')(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  x = layers.Dense(16, activation='relu')(x)\n",
        "  x = layers.Dense(n_classes, name=\"logits\")(x)\n",
        "\n",
        "  model = tf.keras.Model(input_layer, x)\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uUITH9qLZj8",
        "outputId": "7d983002-189e-4ea1-c134-0eab4e9f5552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 16, 49)]     0           []                               \n",
            "                                                                                                  \n",
            " time_distributed (TimeDistribu  (None, 16, 32)      1600        ['input_1[0][0]']                \n",
            " ted)                                                                                             \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 16, 32)       1056        ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 16, 32)       1056        ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " tf.linalg.matmul (TFOpLambda)  (None, 16, 16)       0           ['dense_1[0][0]',                \n",
            "                                                                  'dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.truediv (TFOpLambda)   (None, 16, 16)       0           ['tf.linalg.matmul[0][0]']       \n",
            "                                                                                                  \n",
            " tf.nn.softmax (TFOpLambda)     (None, 16, 16)       0           ['tf.math.truediv[0][0]']        \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 16, 32)       1056        ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_1 (TFOpLambda  (None, 16, 32)      0           ['tf.nn.softmax[0][0]',          \n",
            " )                                                                'dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 512)          0           ['tf.linalg.matmul_1[0][0]']     \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 64)           32832       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 64)           0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 32)           2080        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 32)           0           ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 16)           528         ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " logits (Dense)                 (None, 10)           170         ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 40,378\n",
            "Trainable params: 40,378\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = build_model(X_train[0].shape, 10)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6J0Cs9x0NnIk",
        "outputId": "764978d9-b9a5-4c62-b025-03c142f2b55d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "469/469 [==============================] - 5s 5ms/step - loss: 1.5358 - accuracy: 0.3859 - val_loss: 0.8567 - val_accuracy: 0.7066\n",
            "Epoch 2/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.9262 - accuracy: 0.6419 - val_loss: 0.6733 - val_accuracy: 0.7534\n",
            "Epoch 3/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.7773 - accuracy: 0.7078 - val_loss: 0.6035 - val_accuracy: 0.7723\n",
            "Epoch 4/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.7157 - accuracy: 0.7366 - val_loss: 0.5653 - val_accuracy: 0.7938\n",
            "Epoch 5/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.6741 - accuracy: 0.7563 - val_loss: 0.5407 - val_accuracy: 0.7987\n",
            "Epoch 6/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.6512 - accuracy: 0.7694 - val_loss: 0.5207 - val_accuracy: 0.8147\n",
            "Epoch 7/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.6266 - accuracy: 0.7808 - val_loss: 0.5277 - val_accuracy: 0.8193\n",
            "Epoch 8/100\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.6095 - accuracy: 0.7871 - val_loss: 0.5052 - val_accuracy: 0.8202\n",
            "Epoch 9/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.5953 - accuracy: 0.7939 - val_loss: 0.5010 - val_accuracy: 0.8289\n",
            "Epoch 10/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.5854 - accuracy: 0.7998 - val_loss: 0.5049 - val_accuracy: 0.8258\n",
            "Epoch 11/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.5700 - accuracy: 0.8030 - val_loss: 0.4774 - val_accuracy: 0.8339\n",
            "Epoch 12/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.5635 - accuracy: 0.8043 - val_loss: 0.4961 - val_accuracy: 0.8295\n",
            "Epoch 13/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.5532 - accuracy: 0.8086 - val_loss: 0.4678 - val_accuracy: 0.8323\n",
            "Epoch 14/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.5443 - accuracy: 0.8130 - val_loss: 0.4524 - val_accuracy: 0.8359\n",
            "Epoch 15/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.5366 - accuracy: 0.8148 - val_loss: 0.4635 - val_accuracy: 0.8350\n",
            "Epoch 16/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.5285 - accuracy: 0.8171 - val_loss: 0.4611 - val_accuracy: 0.8376\n",
            "Epoch 17/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.5235 - accuracy: 0.8217 - val_loss: 0.4502 - val_accuracy: 0.8421\n",
            "Epoch 18/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.5222 - accuracy: 0.8202 - val_loss: 0.4478 - val_accuracy: 0.8423\n",
            "Epoch 19/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.5124 - accuracy: 0.8224 - val_loss: 0.4464 - val_accuracy: 0.8399\n",
            "Epoch 20/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.5048 - accuracy: 0.8275 - val_loss: 0.4314 - val_accuracy: 0.8477\n",
            "Epoch 21/100\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.5009 - accuracy: 0.8272 - val_loss: 0.4424 - val_accuracy: 0.8411\n",
            "Epoch 22/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4997 - accuracy: 0.8301 - val_loss: 0.4348 - val_accuracy: 0.8451\n",
            "Epoch 23/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4939 - accuracy: 0.8312 - val_loss: 0.4347 - val_accuracy: 0.8478\n",
            "Epoch 24/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4922 - accuracy: 0.8317 - val_loss: 0.4350 - val_accuracy: 0.8474\n",
            "Epoch 25/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4920 - accuracy: 0.8330 - val_loss: 0.4244 - val_accuracy: 0.8540\n",
            "Epoch 26/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4817 - accuracy: 0.8364 - val_loss: 0.4408 - val_accuracy: 0.8507\n",
            "Epoch 27/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4765 - accuracy: 0.8390 - val_loss: 0.4325 - val_accuracy: 0.8463\n",
            "Epoch 28/100\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.4765 - accuracy: 0.8371 - val_loss: 0.4341 - val_accuracy: 0.8464\n",
            "Epoch 29/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4720 - accuracy: 0.8413 - val_loss: 0.4284 - val_accuracy: 0.8501\n",
            "Epoch 30/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4725 - accuracy: 0.8401 - val_loss: 0.4275 - val_accuracy: 0.8547\n",
            "Epoch 31/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4723 - accuracy: 0.8407 - val_loss: 0.4213 - val_accuracy: 0.8537\n",
            "Epoch 32/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4633 - accuracy: 0.8432 - val_loss: 0.4415 - val_accuracy: 0.8489\n",
            "Epoch 33/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4633 - accuracy: 0.8426 - val_loss: 0.4278 - val_accuracy: 0.8519\n",
            "Epoch 34/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4619 - accuracy: 0.8431 - val_loss: 0.4177 - val_accuracy: 0.8568\n",
            "Epoch 35/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4597 - accuracy: 0.8439 - val_loss: 0.4144 - val_accuracy: 0.8572\n",
            "Epoch 36/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4570 - accuracy: 0.8459 - val_loss: 0.4079 - val_accuracy: 0.8554\n",
            "Epoch 37/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4565 - accuracy: 0.8447 - val_loss: 0.4057 - val_accuracy: 0.8584\n",
            "Epoch 38/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4515 - accuracy: 0.8474 - val_loss: 0.4115 - val_accuracy: 0.8569\n",
            "Epoch 39/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4507 - accuracy: 0.8456 - val_loss: 0.4073 - val_accuracy: 0.8563\n",
            "Epoch 40/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4491 - accuracy: 0.8476 - val_loss: 0.4131 - val_accuracy: 0.8549\n",
            "Epoch 41/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4461 - accuracy: 0.8478 - val_loss: 0.4059 - val_accuracy: 0.8580\n",
            "Epoch 42/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4411 - accuracy: 0.8508 - val_loss: 0.4056 - val_accuracy: 0.8590\n",
            "Epoch 43/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4400 - accuracy: 0.8507 - val_loss: 0.4054 - val_accuracy: 0.8601\n",
            "Epoch 44/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4375 - accuracy: 0.8521 - val_loss: 0.4013 - val_accuracy: 0.8615\n",
            "Epoch 45/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4413 - accuracy: 0.8503 - val_loss: 0.4014 - val_accuracy: 0.8601\n",
            "Epoch 46/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4362 - accuracy: 0.8527 - val_loss: 0.4098 - val_accuracy: 0.8554\n",
            "Epoch 47/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4371 - accuracy: 0.8521 - val_loss: 0.4058 - val_accuracy: 0.8590\n",
            "Epoch 48/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4356 - accuracy: 0.8526 - val_loss: 0.4112 - val_accuracy: 0.8559\n",
            "Epoch 49/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4345 - accuracy: 0.8534 - val_loss: 0.4073 - val_accuracy: 0.8610\n",
            "Epoch 50/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4262 - accuracy: 0.8551 - val_loss: 0.4030 - val_accuracy: 0.8593\n",
            "Epoch 51/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4388 - accuracy: 0.8517 - val_loss: 0.3952 - val_accuracy: 0.8613\n",
            "Epoch 52/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4341 - accuracy: 0.8538 - val_loss: 0.4151 - val_accuracy: 0.8577\n",
            "Epoch 53/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4305 - accuracy: 0.8538 - val_loss: 0.3993 - val_accuracy: 0.8647\n",
            "Epoch 54/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4284 - accuracy: 0.8538 - val_loss: 0.3917 - val_accuracy: 0.8675\n",
            "Epoch 55/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4183 - accuracy: 0.8574 - val_loss: 0.3960 - val_accuracy: 0.8640\n",
            "Epoch 56/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4240 - accuracy: 0.8559 - val_loss: 0.3987 - val_accuracy: 0.8605\n",
            "Epoch 57/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4209 - accuracy: 0.8583 - val_loss: 0.4130 - val_accuracy: 0.8633\n",
            "Epoch 58/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4259 - accuracy: 0.8565 - val_loss: 0.3977 - val_accuracy: 0.8623\n",
            "Epoch 59/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4199 - accuracy: 0.8584 - val_loss: 0.4115 - val_accuracy: 0.8567\n",
            "Epoch 60/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4199 - accuracy: 0.8579 - val_loss: 0.4131 - val_accuracy: 0.8603\n",
            "Epoch 61/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4223 - accuracy: 0.8576 - val_loss: 0.3956 - val_accuracy: 0.8640\n",
            "Epoch 62/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4160 - accuracy: 0.8581 - val_loss: 0.3963 - val_accuracy: 0.8603\n",
            "Epoch 63/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4100 - accuracy: 0.8603 - val_loss: 0.3973 - val_accuracy: 0.8657\n",
            "Epoch 64/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4168 - accuracy: 0.8572 - val_loss: 0.3873 - val_accuracy: 0.8664\n",
            "Epoch 65/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4171 - accuracy: 0.8602 - val_loss: 0.4044 - val_accuracy: 0.8582\n",
            "Epoch 66/100\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.4135 - accuracy: 0.8622 - val_loss: 0.4013 - val_accuracy: 0.8635\n",
            "Epoch 67/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4086 - accuracy: 0.8605 - val_loss: 0.3962 - val_accuracy: 0.8618\n",
            "Epoch 68/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4134 - accuracy: 0.8604 - val_loss: 0.3975 - val_accuracy: 0.8635\n",
            "Epoch 69/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4072 - accuracy: 0.8608 - val_loss: 0.3911 - val_accuracy: 0.8624\n",
            "Epoch 70/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4149 - accuracy: 0.8604 - val_loss: 0.4014 - val_accuracy: 0.8626\n",
            "Epoch 71/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4130 - accuracy: 0.8605 - val_loss: 0.3970 - val_accuracy: 0.8661\n",
            "Epoch 72/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4148 - accuracy: 0.8604 - val_loss: 0.4092 - val_accuracy: 0.8571\n",
            "Epoch 73/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4112 - accuracy: 0.8630 - val_loss: 0.3980 - val_accuracy: 0.8671\n",
            "Epoch 74/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4094 - accuracy: 0.8621 - val_loss: 0.4044 - val_accuracy: 0.8618\n",
            "Epoch 75/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4093 - accuracy: 0.8627 - val_loss: 0.3950 - val_accuracy: 0.8668\n",
            "Epoch 76/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4083 - accuracy: 0.8633 - val_loss: 0.3946 - val_accuracy: 0.8664\n",
            "Epoch 77/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4025 - accuracy: 0.8624 - val_loss: 0.3958 - val_accuracy: 0.8653\n",
            "Epoch 78/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4097 - accuracy: 0.8619 - val_loss: 0.3972 - val_accuracy: 0.8603\n",
            "Epoch 79/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4083 - accuracy: 0.8623 - val_loss: 0.3923 - val_accuracy: 0.8659\n",
            "Epoch 80/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4066 - accuracy: 0.8621 - val_loss: 0.4027 - val_accuracy: 0.8627\n",
            "Epoch 81/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4029 - accuracy: 0.8631 - val_loss: 0.3978 - val_accuracy: 0.8607\n",
            "Epoch 82/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4031 - accuracy: 0.8644 - val_loss: 0.3958 - val_accuracy: 0.8647\n",
            "Epoch 83/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3989 - accuracy: 0.8647 - val_loss: 0.3929 - val_accuracy: 0.8672\n",
            "Epoch 84/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4017 - accuracy: 0.8655 - val_loss: 0.3901 - val_accuracy: 0.8666\n",
            "Epoch 85/100\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.4040 - accuracy: 0.8634 - val_loss: 0.3916 - val_accuracy: 0.8650\n",
            "Epoch 86/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.4033 - accuracy: 0.8641 - val_loss: 0.4034 - val_accuracy: 0.8619\n",
            "Epoch 87/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4011 - accuracy: 0.8651 - val_loss: 0.4100 - val_accuracy: 0.8577\n",
            "Epoch 88/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3974 - accuracy: 0.8660 - val_loss: 0.4100 - val_accuracy: 0.8660\n",
            "Epoch 89/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3993 - accuracy: 0.8648 - val_loss: 0.3949 - val_accuracy: 0.8687\n",
            "Epoch 90/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4017 - accuracy: 0.8644 - val_loss: 0.3860 - val_accuracy: 0.8676\n",
            "Epoch 91/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3973 - accuracy: 0.8651 - val_loss: 0.3968 - val_accuracy: 0.8647\n",
            "Epoch 92/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3941 - accuracy: 0.8661 - val_loss: 0.3859 - val_accuracy: 0.8677\n",
            "Epoch 93/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3936 - accuracy: 0.8679 - val_loss: 0.3934 - val_accuracy: 0.8673\n",
            "Epoch 94/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3936 - accuracy: 0.8670 - val_loss: 0.3919 - val_accuracy: 0.8652\n",
            "Epoch 95/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3917 - accuracy: 0.8668 - val_loss: 0.3868 - val_accuracy: 0.8654\n",
            "Epoch 96/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3965 - accuracy: 0.8671 - val_loss: 0.3947 - val_accuracy: 0.8641\n",
            "Epoch 97/100\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3958 - accuracy: 0.8681 - val_loss: 0.3911 - val_accuracy: 0.8657\n",
            "Epoch 98/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3891 - accuracy: 0.8676 - val_loss: 0.3956 - val_accuracy: 0.8644\n",
            "Epoch 99/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3907 - accuracy: 0.8675 - val_loss: 0.3896 - val_accuracy: 0.8683\n",
            "Epoch 100/100\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.3909 - accuracy: 0.8681 - val_loss: 0.3950 - val_accuracy: 0.8664\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        loss  accuracy  val_loss  val_accuracy\n",
              "0   1.535806  0.385950  0.856680        0.7066\n",
              "1   0.926190  0.641917  0.673257        0.7534\n",
              "2   0.777326  0.707833  0.603454        0.7723\n",
              "3   0.715739  0.736650  0.565322        0.7938\n",
              "4   0.674116  0.756317  0.540693        0.7987\n",
              "..       ...       ...       ...           ...\n",
              "95  0.396450  0.867133  0.394682        0.8641\n",
              "96  0.395802  0.868083  0.391094        0.8657\n",
              "97  0.389076  0.867600  0.395609        0.8644\n",
              "98  0.390662  0.867483  0.389567        0.8683\n",
              "99  0.390896  0.868133  0.394986        0.8664\n",
              "\n",
              "[100 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-37e87943-424e-4fe7-b7a1-8586b18c05e8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.535806</td>\n",
              "      <td>0.385950</td>\n",
              "      <td>0.856680</td>\n",
              "      <td>0.7066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.926190</td>\n",
              "      <td>0.641917</td>\n",
              "      <td>0.673257</td>\n",
              "      <td>0.7534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.777326</td>\n",
              "      <td>0.707833</td>\n",
              "      <td>0.603454</td>\n",
              "      <td>0.7723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.715739</td>\n",
              "      <td>0.736650</td>\n",
              "      <td>0.565322</td>\n",
              "      <td>0.7938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.674116</td>\n",
              "      <td>0.756317</td>\n",
              "      <td>0.540693</td>\n",
              "      <td>0.7987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.396450</td>\n",
              "      <td>0.867133</td>\n",
              "      <td>0.394682</td>\n",
              "      <td>0.8641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.395802</td>\n",
              "      <td>0.868083</td>\n",
              "      <td>0.391094</td>\n",
              "      <td>0.8657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.389076</td>\n",
              "      <td>0.867600</td>\n",
              "      <td>0.395609</td>\n",
              "      <td>0.8644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.390662</td>\n",
              "      <td>0.867483</td>\n",
              "      <td>0.389567</td>\n",
              "      <td>0.8683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.390896</td>\n",
              "      <td>0.868133</td>\n",
              "      <td>0.394986</td>\n",
              "      <td>0.8664</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37e87943-424e-4fe7-b7a1-8586b18c05e8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-37e87943-424e-4fe7-b7a1-8586b18c05e8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-37e87943-424e-4fe7-b7a1-8586b18c05e8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Initiate Stochastic Gradient Descent optimizer\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(loss=loss,\n",
        "            optimizer=opt,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "# Fit the model.\n",
        "history = model.fit(\n",
        "  x = X_train,  \n",
        "  y = Y_train,  \n",
        "  epochs=100,     \n",
        "  batch_size=128, \n",
        "  # validation_split=0.1,\n",
        "  validation_data=(X_test, Y_test),\n",
        "  verbose=1            \n",
        "  )\n",
        "\n",
        "# Convert the return value into a DataFrame so we can see the train loss \n",
        "# and binary accuracy after every epoch.\n",
        "history = pd.DataFrame(history.history)\n",
        "display(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghbb2ZsR8vAP"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qCR9OD0yclt3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_attention_head(embeddings, id):\n",
        "  emb_dim = 32\n",
        "  query_key_dim = 32\n",
        "  value_dim = 32\n",
        "\n",
        "  query = layers.Dense(query_key_dim)(embeddings)\n",
        "  key = layers.Dense(query_key_dim)(embeddings)\n",
        "  value = layers.Dense(value_dim)(embeddings)\n",
        "\n",
        "  attention_scores = tf.linalg.matmul(query, key, transpose_b=True)\n",
        "  attention_scores = attention_scores / tf.math.sqrt(float(query_key_dim))\n",
        "\n",
        "  attention_scores = tf.nn.softmax(attention_scores, axis=-1)\n",
        "\n",
        "  attention_head_out = tf.linalg.matmul(attention_scores, value)\n",
        "\n",
        "  return layers.Flatten()(attention_head_out)\n",
        "\n",
        "def build_multihead_model(input_shape, n_classes):\n",
        "\n",
        "  # Create the model\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  input_layer = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "  # Extract embeddings of size emb_dim from each patch\n",
        "  emb_extractor = layers.Dense(emb_dim)\n",
        "  embeddings = layers.TimeDistributed(emb_extractor)(input_layer)\n",
        "  # embeddings = input_layer\n",
        "\n",
        "  att_heads = []\n",
        "  for i in range(8):\n",
        "    att_heads.append(build_attention_head(embeddings, str(i)))\n",
        "  x = tf.concat(att_heads, axis=-1)\n",
        "\n",
        "  x = layers.Dense(64, activation='relu')(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  x = layers.Dense(32, activation='relu')(x)\n",
        "  x = layers.Dense(n_classes, name=\"logits\")(x)\n",
        "\n",
        "  model = tf.keras.Model(input_layer, x)\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2px7tgMjeoj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf3b4985-f1d8-476c-91f6-17189276e894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 16, 49)]     0           []                               \n",
            "                                                                                                  \n",
            " time_distributed (TimeDistribu  (None, 16, 3)       150         ['input_1[0][0]']                \n",
            " ted)                                                                                             \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " dense_23 (Dense)               (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " tf.linalg.matmul (TFOpLambda)  (None, 16, 16)       0           ['dense_1[0][0]',                \n",
            "                                                                  'dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_2 (TFOpLambda  (None, 16, 16)      0           ['dense_4[0][0]',                \n",
            " )                                                                'dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_4 (TFOpLambda  (None, 16, 16)      0           ['dense_7[0][0]',                \n",
            " )                                                                'dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_6 (TFOpLambda  (None, 16, 16)      0           ['dense_10[0][0]',               \n",
            " )                                                                'dense_11[0][0]']               \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_8 (TFOpLambda  (None, 16, 16)      0           ['dense_13[0][0]',               \n",
            " )                                                                'dense_14[0][0]']               \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_10 (TFOpLambd  (None, 16, 16)      0           ['dense_16[0][0]',               \n",
            " a)                                                               'dense_17[0][0]']               \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_12 (TFOpLambd  (None, 16, 16)      0           ['dense_19[0][0]',               \n",
            " a)                                                               'dense_20[0][0]']               \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_14 (TFOpLambd  (None, 16, 16)      0           ['dense_22[0][0]',               \n",
            " a)                                                               'dense_23[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.truediv (TFOpLambda)   (None, 16, 16)       0           ['tf.linalg.matmul[0][0]']       \n",
            "                                                                                                  \n",
            " tf.math.truediv_1 (TFOpLambda)  (None, 16, 16)      0           ['tf.linalg.matmul_2[0][0]']     \n",
            "                                                                                                  \n",
            " tf.math.truediv_2 (TFOpLambda)  (None, 16, 16)      0           ['tf.linalg.matmul_4[0][0]']     \n",
            "                                                                                                  \n",
            " tf.math.truediv_3 (TFOpLambda)  (None, 16, 16)      0           ['tf.linalg.matmul_6[0][0]']     \n",
            "                                                                                                  \n",
            " tf.math.truediv_4 (TFOpLambda)  (None, 16, 16)      0           ['tf.linalg.matmul_8[0][0]']     \n",
            "                                                                                                  \n",
            " tf.math.truediv_5 (TFOpLambda)  (None, 16, 16)      0           ['tf.linalg.matmul_10[0][0]']    \n",
            "                                                                                                  \n",
            " tf.math.truediv_6 (TFOpLambda)  (None, 16, 16)      0           ['tf.linalg.matmul_12[0][0]']    \n",
            "                                                                                                  \n",
            " tf.math.truediv_7 (TFOpLambda)  (None, 16, 16)      0           ['tf.linalg.matmul_14[0][0]']    \n",
            "                                                                                                  \n",
            " tf.nn.softmax (TFOpLambda)     (None, 16, 16)       0           ['tf.math.truediv[0][0]']        \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " tf.nn.softmax_1 (TFOpLambda)   (None, 16, 16)       0           ['tf.math.truediv_1[0][0]']      \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " tf.nn.softmax_2 (TFOpLambda)   (None, 16, 16)       0           ['tf.math.truediv_2[0][0]']      \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " tf.nn.softmax_3 (TFOpLambda)   (None, 16, 16)       0           ['tf.math.truediv_3[0][0]']      \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " tf.nn.softmax_4 (TFOpLambda)   (None, 16, 16)       0           ['tf.math.truediv_4[0][0]']      \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " tf.nn.softmax_5 (TFOpLambda)   (None, 16, 16)       0           ['tf.math.truediv_5[0][0]']      \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " tf.nn.softmax_6 (TFOpLambda)   (None, 16, 16)       0           ['tf.math.truediv_6[0][0]']      \n",
            "                                                                                                  \n",
            " dense_21 (Dense)               (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " tf.nn.softmax_7 (TFOpLambda)   (None, 16, 16)       0           ['tf.math.truediv_7[0][0]']      \n",
            "                                                                                                  \n",
            " dense_24 (Dense)               (None, 16, 32)       128         ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_1 (TFOpLambda  (None, 16, 32)      0           ['tf.nn.softmax[0][0]',          \n",
            " )                                                                'dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_3 (TFOpLambda  (None, 16, 32)      0           ['tf.nn.softmax_1[0][0]',        \n",
            " )                                                                'dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_5 (TFOpLambda  (None, 16, 32)      0           ['tf.nn.softmax_2[0][0]',        \n",
            " )                                                                'dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_7 (TFOpLambda  (None, 16, 32)      0           ['tf.nn.softmax_3[0][0]',        \n",
            " )                                                                'dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_9 (TFOpLambda  (None, 16, 32)      0           ['tf.nn.softmax_4[0][0]',        \n",
            " )                                                                'dense_15[0][0]']               \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_11 (TFOpLambd  (None, 16, 32)      0           ['tf.nn.softmax_5[0][0]',        \n",
            " a)                                                               'dense_18[0][0]']               \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_13 (TFOpLambd  (None, 16, 32)      0           ['tf.nn.softmax_6[0][0]',        \n",
            " a)                                                               'dense_21[0][0]']               \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_15 (TFOpLambd  (None, 16, 32)      0           ['tf.nn.softmax_7[0][0]',        \n",
            " a)                                                               'dense_24[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 512)          0           ['tf.linalg.matmul_1[0][0]']     \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 512)          0           ['tf.linalg.matmul_3[0][0]']     \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 512)          0           ['tf.linalg.matmul_5[0][0]']     \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 512)          0           ['tf.linalg.matmul_7[0][0]']     \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)            (None, 512)          0           ['tf.linalg.matmul_9[0][0]']     \n",
            "                                                                                                  \n",
            " flatten_5 (Flatten)            (None, 512)          0           ['tf.linalg.matmul_11[0][0]']    \n",
            "                                                                                                  \n",
            " flatten_6 (Flatten)            (None, 512)          0           ['tf.linalg.matmul_13[0][0]']    \n",
            "                                                                                                  \n",
            " flatten_7 (Flatten)            (None, 512)          0           ['tf.linalg.matmul_15[0][0]']    \n",
            "                                                                                                  \n",
            " tf.concat (TFOpLambda)         (None, 4096)         0           ['flatten[0][0]',                \n",
            "                                                                  'flatten_1[0][0]',              \n",
            "                                                                  'flatten_2[0][0]',              \n",
            "                                                                  'flatten_3[0][0]',              \n",
            "                                                                  'flatten_4[0][0]',              \n",
            "                                                                  'flatten_5[0][0]',              \n",
            "                                                                  'flatten_6[0][0]',              \n",
            "                                                                  'flatten_7[0][0]']              \n",
            "                                                                                                  \n",
            " dense_25 (Dense)               (None, 64)           262208      ['tf.concat[0][0]']              \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 64)           0           ['dense_25[0][0]']               \n",
            "                                                                                                  \n",
            " dense_26 (Dense)               (None, 32)           2080        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " logits (Dense)                 (None, 10)           330         ['dense_26[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 267,840\n",
            "Trainable params: 267,840\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = build_multihead_model(X_train[0].shape, 10)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UovzNjWffxp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9b1b326-1fe3-466d-cd7a-f273ad7f1176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "469/469 [==============================] - 8s 10ms/step - loss: 1.3899 - accuracy: 0.4746 - val_loss: 0.7370 - val_accuracy: 0.7083\n",
            "Epoch 2/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.7861 - accuracy: 0.7082 - val_loss: 0.6391 - val_accuracy: 0.7590\n",
            "Epoch 3/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.6898 - accuracy: 0.7467 - val_loss: 0.5746 - val_accuracy: 0.7808\n",
            "Epoch 4/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.6359 - accuracy: 0.7675 - val_loss: 0.5352 - val_accuracy: 0.8083\n",
            "Epoch 5/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.5949 - accuracy: 0.7819 - val_loss: 0.5271 - val_accuracy: 0.8138\n",
            "Epoch 6/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.5673 - accuracy: 0.7933 - val_loss: 0.5111 - val_accuracy: 0.8242\n",
            "Epoch 7/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.5421 - accuracy: 0.8020 - val_loss: 0.4983 - val_accuracy: 0.8210\n",
            "Epoch 8/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.5324 - accuracy: 0.8071 - val_loss: 0.4922 - val_accuracy: 0.8276\n",
            "Epoch 9/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.5182 - accuracy: 0.8095 - val_loss: 0.5045 - val_accuracy: 0.8168\n",
            "Epoch 10/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.5048 - accuracy: 0.8141 - val_loss: 0.4859 - val_accuracy: 0.8239\n",
            "Epoch 11/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4963 - accuracy: 0.8181 - val_loss: 0.4606 - val_accuracy: 0.8350\n",
            "Epoch 12/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4830 - accuracy: 0.8234 - val_loss: 0.4726 - val_accuracy: 0.8372\n",
            "Epoch 13/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.4750 - accuracy: 0.8261 - val_loss: 0.4874 - val_accuracy: 0.8198\n",
            "Epoch 14/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4663 - accuracy: 0.8275 - val_loss: 0.4579 - val_accuracy: 0.8380\n",
            "Epoch 15/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4616 - accuracy: 0.8305 - val_loss: 0.4752 - val_accuracy: 0.8328\n",
            "Epoch 16/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4510 - accuracy: 0.8354 - val_loss: 0.4706 - val_accuracy: 0.8298\n",
            "Epoch 17/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4534 - accuracy: 0.8337 - val_loss: 0.4857 - val_accuracy: 0.8235\n",
            "Epoch 18/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4432 - accuracy: 0.8372 - val_loss: 0.4561 - val_accuracy: 0.8371\n",
            "Epoch 19/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4387 - accuracy: 0.8393 - val_loss: 0.4622 - val_accuracy: 0.8408\n",
            "Epoch 20/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4321 - accuracy: 0.8410 - val_loss: 0.4529 - val_accuracy: 0.8336\n",
            "Epoch 21/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4327 - accuracy: 0.8411 - val_loss: 0.4503 - val_accuracy: 0.8353\n",
            "Epoch 22/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4287 - accuracy: 0.8416 - val_loss: 0.4784 - val_accuracy: 0.8356\n",
            "Epoch 23/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4252 - accuracy: 0.8431 - val_loss: 0.4459 - val_accuracy: 0.8445\n",
            "Epoch 24/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4255 - accuracy: 0.8434 - val_loss: 0.4433 - val_accuracy: 0.8449\n",
            "Epoch 25/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.4214 - accuracy: 0.8441 - val_loss: 0.4468 - val_accuracy: 0.8378\n",
            "Epoch 26/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4178 - accuracy: 0.8452 - val_loss: 0.4347 - val_accuracy: 0.8489\n",
            "Epoch 27/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4157 - accuracy: 0.8468 - val_loss: 0.4331 - val_accuracy: 0.8498\n",
            "Epoch 28/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4151 - accuracy: 0.8459 - val_loss: 0.4607 - val_accuracy: 0.8304\n",
            "Epoch 29/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4107 - accuracy: 0.8482 - val_loss: 0.4592 - val_accuracy: 0.8297\n",
            "Epoch 30/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4057 - accuracy: 0.8496 - val_loss: 0.4351 - val_accuracy: 0.8459\n",
            "Epoch 31/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4049 - accuracy: 0.8498 - val_loss: 0.4438 - val_accuracy: 0.8449\n",
            "Epoch 32/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4021 - accuracy: 0.8499 - val_loss: 0.4533 - val_accuracy: 0.8360\n",
            "Epoch 33/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.4002 - accuracy: 0.8498 - val_loss: 0.4557 - val_accuracy: 0.8377\n",
            "Epoch 34/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3982 - accuracy: 0.8521 - val_loss: 0.4460 - val_accuracy: 0.8443\n",
            "Epoch 35/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3979 - accuracy: 0.8528 - val_loss: 0.4366 - val_accuracy: 0.8468\n",
            "Epoch 36/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3968 - accuracy: 0.8526 - val_loss: 0.4556 - val_accuracy: 0.8304\n",
            "Epoch 37/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3958 - accuracy: 0.8521 - val_loss: 0.4309 - val_accuracy: 0.8465\n",
            "Epoch 38/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3955 - accuracy: 0.8515 - val_loss: 0.4233 - val_accuracy: 0.8507\n",
            "Epoch 39/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3921 - accuracy: 0.8528 - val_loss: 0.4409 - val_accuracy: 0.8383\n",
            "Epoch 40/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3892 - accuracy: 0.8553 - val_loss: 0.4364 - val_accuracy: 0.8458\n",
            "Epoch 41/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3903 - accuracy: 0.8541 - val_loss: 0.4407 - val_accuracy: 0.8425\n",
            "Epoch 42/100\n",
            "469/469 [==============================] - 4s 7ms/step - loss: 0.3886 - accuracy: 0.8544 - val_loss: 0.4206 - val_accuracy: 0.8517\n",
            "Epoch 43/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3850 - accuracy: 0.8553 - val_loss: 0.4179 - val_accuracy: 0.8531\n",
            "Epoch 44/100\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.3842 - accuracy: 0.8572 - val_loss: 0.4280 - val_accuracy: 0.8482\n",
            "Epoch 45/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3866 - accuracy: 0.8557 - val_loss: 0.4375 - val_accuracy: 0.8421\n",
            "Epoch 46/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3828 - accuracy: 0.8589 - val_loss: 0.4270 - val_accuracy: 0.8460\n",
            "Epoch 47/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3826 - accuracy: 0.8568 - val_loss: 0.4254 - val_accuracy: 0.8506\n",
            "Epoch 48/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3807 - accuracy: 0.8573 - val_loss: 0.4485 - val_accuracy: 0.8377\n",
            "Epoch 49/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3796 - accuracy: 0.8589 - val_loss: 0.4487 - val_accuracy: 0.8424\n",
            "Epoch 50/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3824 - accuracy: 0.8577 - val_loss: 0.4572 - val_accuracy: 0.8317\n",
            "Epoch 51/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3796 - accuracy: 0.8586 - val_loss: 0.4421 - val_accuracy: 0.8457\n",
            "Epoch 52/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3806 - accuracy: 0.8584 - val_loss: 0.4371 - val_accuracy: 0.8453\n",
            "Epoch 53/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3735 - accuracy: 0.8598 - val_loss: 0.4352 - val_accuracy: 0.8477\n",
            "Epoch 54/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3761 - accuracy: 0.8599 - val_loss: 0.4246 - val_accuracy: 0.8435\n",
            "Epoch 55/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3737 - accuracy: 0.8601 - val_loss: 0.4321 - val_accuracy: 0.8456\n",
            "Epoch 56/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3744 - accuracy: 0.8592 - val_loss: 0.4345 - val_accuracy: 0.8442\n",
            "Epoch 57/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3722 - accuracy: 0.8615 - val_loss: 0.4512 - val_accuracy: 0.8368\n",
            "Epoch 58/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3722 - accuracy: 0.8636 - val_loss: 0.4134 - val_accuracy: 0.8516\n",
            "Epoch 59/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3730 - accuracy: 0.8605 - val_loss: 0.4144 - val_accuracy: 0.8488\n",
            "Epoch 60/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3713 - accuracy: 0.8621 - val_loss: 0.4313 - val_accuracy: 0.8429\n",
            "Epoch 61/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3714 - accuracy: 0.8605 - val_loss: 0.4149 - val_accuracy: 0.8516\n",
            "Epoch 62/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3683 - accuracy: 0.8615 - val_loss: 0.4251 - val_accuracy: 0.8537\n",
            "Epoch 63/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3697 - accuracy: 0.8619 - val_loss: 0.4394 - val_accuracy: 0.8432\n",
            "Epoch 64/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.3692 - accuracy: 0.8622 - val_loss: 0.4204 - val_accuracy: 0.8513\n",
            "Epoch 65/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.3699 - accuracy: 0.8622 - val_loss: 0.4176 - val_accuracy: 0.8521\n",
            "Epoch 66/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3633 - accuracy: 0.8627 - val_loss: 0.4187 - val_accuracy: 0.8509\n",
            "Epoch 67/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3671 - accuracy: 0.8629 - val_loss: 0.4267 - val_accuracy: 0.8483\n",
            "Epoch 68/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3648 - accuracy: 0.8621 - val_loss: 0.4179 - val_accuracy: 0.8514\n",
            "Epoch 69/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3648 - accuracy: 0.8644 - val_loss: 0.4164 - val_accuracy: 0.8538\n",
            "Epoch 70/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3664 - accuracy: 0.8631 - val_loss: 0.4323 - val_accuracy: 0.8484\n",
            "Epoch 71/100\n",
            "469/469 [==============================] - 4s 7ms/step - loss: 0.3635 - accuracy: 0.8641 - val_loss: 0.4284 - val_accuracy: 0.8496\n",
            "Epoch 72/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3602 - accuracy: 0.8649 - val_loss: 0.4238 - val_accuracy: 0.8447\n",
            "Epoch 73/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3628 - accuracy: 0.8646 - val_loss: 0.4476 - val_accuracy: 0.8399\n",
            "Epoch 74/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3607 - accuracy: 0.8641 - val_loss: 0.4143 - val_accuracy: 0.8510\n",
            "Epoch 75/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3609 - accuracy: 0.8646 - val_loss: 0.4347 - val_accuracy: 0.8448\n",
            "Epoch 76/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3597 - accuracy: 0.8658 - val_loss: 0.4440 - val_accuracy: 0.8399\n",
            "Epoch 77/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3598 - accuracy: 0.8669 - val_loss: 0.4363 - val_accuracy: 0.8442\n",
            "Epoch 78/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3597 - accuracy: 0.8656 - val_loss: 0.4320 - val_accuracy: 0.8435\n",
            "Epoch 79/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3590 - accuracy: 0.8647 - val_loss: 0.4352 - val_accuracy: 0.8429\n",
            "Epoch 80/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3581 - accuracy: 0.8651 - val_loss: 0.4336 - val_accuracy: 0.8467\n",
            "Epoch 81/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3590 - accuracy: 0.8650 - val_loss: 0.4480 - val_accuracy: 0.8368\n",
            "Epoch 82/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3576 - accuracy: 0.8664 - val_loss: 0.4464 - val_accuracy: 0.8410\n",
            "Epoch 83/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3577 - accuracy: 0.8657 - val_loss: 0.4280 - val_accuracy: 0.8462\n",
            "Epoch 84/100\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.3564 - accuracy: 0.8674 - val_loss: 0.4355 - val_accuracy: 0.8415\n",
            "Epoch 85/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.3544 - accuracy: 0.8662 - val_loss: 0.4535 - val_accuracy: 0.8334\n",
            "Epoch 86/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3509 - accuracy: 0.8688 - val_loss: 0.4239 - val_accuracy: 0.8467\n",
            "Epoch 87/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3541 - accuracy: 0.8683 - val_loss: 0.4098 - val_accuracy: 0.8535\n",
            "Epoch 88/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3546 - accuracy: 0.8667 - val_loss: 0.4144 - val_accuracy: 0.8502\n",
            "Epoch 89/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3540 - accuracy: 0.8666 - val_loss: 0.4319 - val_accuracy: 0.8437\n",
            "Epoch 90/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3578 - accuracy: 0.8648 - val_loss: 0.4238 - val_accuracy: 0.8471\n",
            "Epoch 91/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3530 - accuracy: 0.8671 - val_loss: 0.4383 - val_accuracy: 0.8479\n",
            "Epoch 92/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3542 - accuracy: 0.8665 - val_loss: 0.4213 - val_accuracy: 0.8516\n",
            "Epoch 93/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.3515 - accuracy: 0.8699 - val_loss: 0.4133 - val_accuracy: 0.8507\n",
            "Epoch 94/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.3517 - accuracy: 0.8676 - val_loss: 0.4314 - val_accuracy: 0.8474\n",
            "Epoch 95/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.3502 - accuracy: 0.8688 - val_loss: 0.4360 - val_accuracy: 0.8440\n",
            "Epoch 96/100\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.3505 - accuracy: 0.8680 - val_loss: 0.4323 - val_accuracy: 0.8396\n",
            "Epoch 97/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3481 - accuracy: 0.8701 - val_loss: 0.4334 - val_accuracy: 0.8422\n",
            "Epoch 98/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3524 - accuracy: 0.8676 - val_loss: 0.4158 - val_accuracy: 0.8497\n",
            "Epoch 99/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3494 - accuracy: 0.8688 - val_loss: 0.4169 - val_accuracy: 0.8490\n",
            "Epoch 100/100\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.3484 - accuracy: 0.8694 - val_loss: 0.4197 - val_accuracy: 0.8481\n"
          ]
        }
      ],
      "source": [
        "# Initiate Stochastic Gradient Descent optimizer\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(loss=loss,\n",
        "            optimizer=opt,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "# Fit the model.\n",
        "history = model.fit(\n",
        "  x = X_train,  \n",
        "  y = Y_train,  \n",
        "  epochs=100,     \n",
        "  batch_size=128, \n",
        "  # validation_split=0.1,\n",
        "  validation_data=(X_test, Y_test),\n",
        "  verbose=1            \n",
        "  )\n",
        "\n",
        "# Convert the return value into a DataFrame so we can see the train loss \n",
        "# and binary accuracy after every epoch.\n",
        "history = pd.DataFrame(history.history)\n",
        "display(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1o47If3M4ZW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmmpZ_W1bJQb"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}