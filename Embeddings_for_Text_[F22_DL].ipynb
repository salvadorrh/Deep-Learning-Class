{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Before you start\n",
        "Make a copy of this Colab by clicking on File > Save a Copy in Drive\n",
        "Input your student id, first name, and last name"
      ],
      "metadata": {
        "id": "i_i_Aum_QQBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  { run: \"auto\", display-mode: \"form\" }\n",
        "student_id = \"80683116\" #@param {type:\"string\"}\n",
        "first_name = \"Salvador\" #@param {type:\"string\"}\n",
        "last_name = \"Robles Herrera\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "FH5xoZVmQR7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43534tdfgs-v"
      },
      "source": [
        "In this lab, we'll train models for sentiment classification and experiment with learned embeddings for text features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X58hOMTUH-w"
      },
      "source": [
        "# Import the libraries we'll use below.\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns  # for nicer plots\n",
        "sns.set(style=\"darkgrid\")  # default style\n",
        "import plotly.graph_objs as plotly  # for interactive plots\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqppUDpmdptk"
      },
      "source": [
        "## Data for Sentiment Classification\n",
        "\n",
        "Let's train a *sentiment* classifier for movie reviews. That is, the input is the text of a movie review and the output is the probability the input was a positive review. The target labels are binary, 0 for negative and 1 for positive.\n",
        "\n",
        "Our data includes 50,000 movie reviews on IMDB. The data comes pre-segmented into train and test splits. The [data loading function](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data) below also splits each input text into tokens (words) and maps the words to integer values. Each input is a sequence of integers corresponding to the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6M-asvhQWV_"
      },
      "source": [
        "# Goal: See if a movie review is positive or negative\n",
        "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(path=\"imdb.npz\",\n",
        "                                                      num_words=None,\n",
        "                                                      skip_top=0,\n",
        "                                                      maxlen=None,\n",
        "                                                      seed=113,\n",
        "                                                      start_char=1,\n",
        "                                                      oov_char=2,\n",
        "                                                      index_from=3)\n",
        "\n",
        "print(\"X_train.shape:\", X_train.shape)\n",
        "print(\"Y_train.shape:\", Y_train.shape)\n",
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"Y_test.shape:\", Y_test.shape)\n",
        "\n",
        "print('First training example data:', X_train[0])\n",
        "print('First training example label:', Y_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyIWiy-4gQK-"
      },
      "source": [
        "So our first training example is a positive review. But that sequence of integer IDs is hard to read. The data loader provides a dictionary mapping words to IDs. Let's create a reverse index.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ-qATkhUj7c"
      },
      "source": [
        "\"\"\"\n",
        "Mapping ids back to words\n",
        "\"\"\"\n",
        "\n",
        "# The imdb dataset comes with an index mapping words to integers.\n",
        "# In the index the words are ordered by frequency they occur.\n",
        "index = imdb.get_word_index()\n",
        "\n",
        "# Because we used index_from=3 (above), setting aside ids below 3 for special\n",
        "# symbols, we need to add 3 to the index values.\n",
        "index = dict([(key, value+3) for (key, value) in index.items()])\n",
        "\n",
        "# Create a reverse index so we can lookup tokens assigned to each id.\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()])\n",
        "reverse_index[1] = '<START>'  # start of input\n",
        "reverse_index[2] = '#'        # out-of-vocabulary (OOV)\n",
        "reverse_index[3] = '<UNUSED>'\n",
        "\n",
        "max_id = max(reverse_index.keys())\n",
        "print('Largest ID:', max_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h76-b07ehWNQ"
      },
      "source": [
        "Note that our index (and reverse index) have over 88,000 tokens. That's quite a large vocabulary! Let's also write a decoding function for our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjobmouHS5Dm"
      },
      "source": [
        "# Given a list of nums return list of words\n",
        "def decode(token_ids):\n",
        "  \"\"\"Return a string with the decoded text given a list of token ids.\"\"\"\n",
        "  # Try looking up each id in the index, but return '#' (for OOV) if not found.\n",
        "  tokens = [reverse_index.get(i, \"#\") for i in token_ids]\n",
        "\n",
        "  # Connect the string tokens with a space.\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "# Show the ids corresponding tokens in the first example.\n",
        "print(X_train[0])\n",
        "print(decode(X_train[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode([16, 1])"
      ],
      "metadata": {
        "id": "CJuZFllbq2lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g47w5CackGBA"
      },
      "source": [
        "### Text Lengths\n",
        "As usual, let's start with some data analysis. How long are the reviews? Is there a difference in length between positive and negative reviews? A histogram will help answer these questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEOgzo8Gk3r7"
      },
      "source": [
        "\"\"\"\n",
        "Distribution of lengths\n",
        "\"\"\"\n",
        "\n",
        "# Create a list of lengths for training examples with a positive label.\n",
        "text_lengths_pos = [len(x) for (i, x) in enumerate(X_train) if Y_train[i]]\n",
        "\n",
        "# And a list of lengths for training examples with a negative label.\n",
        "text_lengths_neg = [len(x) for (i, x) in enumerate(X_train) if not Y_train[i]]\n",
        "\n",
        "# The histogram function can take a list of inputs and corresponding labels.\n",
        "plt.hist([text_lengths_pos, text_lengths_neg], bins=20, range=(0, 1000),\n",
        "         label=['positive', 'negative'])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Also check the longest reviews.\n",
        "print('Longest positive review:', max(text_lengths_pos))\n",
        "print('Longest negative review:', max(text_lengths_neg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3ZE9gpkml3a"
      },
      "source": [
        "### Token Counts \n",
        "For each of the tokens below, let's construct a table with the number of positive training examples that include that token and the number of negative training examples that include that token. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YOYo6d01aWI"
      },
      "source": [
        "tokens = ['good', 'bad', 'amazing', 'boring', 'laugh', 'cry']\n",
        "print('%10s %5s %5s' %('Token', 'Pos', 'Neg'))\n",
        "for token in tokens:\n",
        "  token_id = index[token]  \n",
        "  pos_count = sum([token_id in X_train[i] for i in range(25000) if Y_train[i]])\n",
        "  neg_count = sum([token_id in X_train[i] for i in range(25000)\n",
        "                    if not Y_train[i]])\n",
        "  print('%10s %5d %5d' %(token, pos_count, neg_count))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhzt-LnQ1m8w"
      },
      "source": [
        "## Feature Representation\n",
        "Consider the difference between the pixel features we used for image classification and the text features we are now dealing with.\n",
        "\n",
        "An image had 784 pixel positions. At each position, there is a single value in [0,1] (after normalization).\n",
        "\n",
        "In contrast, a review has a variable number of ordered tokens (up to 2494 in the training examples). Each token occurs in a particular position. We can think of the token positions much like the 784 pixel positions, except that some of the trailing positions are empty, since review lengths vary.  At each token position, there is a single token, one of the 88587 entries in the vocabulary. So we can think of a review as a (2500, 90000) matrix: At each of ~2500 token positions, we have 1 of ~90000 token ids.\n",
        "\n",
        "This representation would have 2500 * 90000 = 225 million features -- quite a lot more complexity than the images, though as you'll see below, we will make some simplifying assumptions, reducing both the number of token positions and the number of vocabulary items."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm_F5JmWyfko"
      },
      "source": [
        "### Padding and Reduced Length\n",
        "As is clear from the length histogram, the current representation of the review text is a variable-length array. Since fixed-length arrays are easier to work with in Tensorflow, let's add special padding tokens at the end of each review until they are all the same length.\n",
        "\n",
        "We'll also use this operation to limit the number of token positions by truncating all reviews to a specified length. In the code below, as an example, we pad all training inputs to length 300."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4ou8bSUCWOx"
      },
      "source": [
        "def pad_data(sequences, max_length):\n",
        "  # Keras has a convenient utility for padding a sequence.\n",
        "  # Also make sure we get a numpy array rather than an array of lists.\n",
        "  return np.array(list(\n",
        "      tf.keras.preprocessing.sequence.pad_sequences(\n",
        "          sequences, maxlen=max_length, padding='post', value=0)))\n",
        "\n",
        "# Pad and truncate to 300 tokens.\n",
        "X_train_padded = pad_data(X_train, max_length=300)\n",
        "\n",
        "# Check the padded output.\n",
        "print('Length of X_train[0]:', len(X_train[0]))\n",
        "print('Length of X_train_padded[0]:', len(X_train_padded[0]))\n",
        "print(X_train_padded[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFEmcwBjL4e_"
      },
      "source": [
        "### Reduced Vocabulary\n",
        "We also want to be able to limit the vocabulary size. Since our padding function produces fixed-length sequences in a numpy matrix, we can use clever numpy indexing to efficiently replace all token ids larger than some value with the designated out-of-vocabulary (OOV) id.\n",
        "\n",
        "In the code below, as an example, we'll keep just token ids less than 1000, replacing all others with OOV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21qpyEgGNQeB"
      },
      "source": [
        "def limit_vocab(sequences, max_token_id, oov_id=2):\n",
        "  \"\"\"Replace token ids greater than or equal to max_token_id with the oov_id.\"\"\"\n",
        "  reduced_sequences = np.copy(sequences)\n",
        "  reduced_sequences[reduced_sequences >= max_token_id] = oov_id\n",
        "  return reduced_sequences\n",
        "\n",
        "# Reduce vocabulary to 1000 tokens.\n",
        "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
        "print(X_train_reduced[0])\n",
        "\n",
        "# Decode to see what this looks like in tokens. Note the '#' for OOVs.\n",
        "print(decode(X_train_reduced[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d24mOPC6ybC4"
      },
      "source": [
        "### One-hot Encoding\n",
        "Our current feature representations are **sparse**. That is, we only keep track of the token ids that are present in the input. A **one-hot** encoding replaces a value like 22 (corresponding to 'film') with an array with a single 1 at position 22 and zeros everywhere else. This will be very memory-inefficient, but we'll do it anyway for clarity.\n",
        "\n",
        "As discussed above, let's dramatically reduce both the number of token positions (review length) and the number of token ids (vocabulary). We'll clip each review after 20 tokens (so 2500 -> 20) and keep only the most common 1000 tokens (so 90000 -> 1000)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXzkqVL3Jufj"
      },
      "source": [
        "# Note that there is only 1000 tokens, the most common\n",
        "# 20 words for each review.\n",
        "# (25000, 20, 1000) -> 25000 reviews with 20 words each and 1000 tokens\n",
        "# The one-hot-encoding of length 1000\n",
        "\n",
        "# Keras has a util to create one-hot encodings.\n",
        "X_train_padded = pad_data(X_train, max_length=20)\n",
        "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
        "X_train_one_hot = tf.keras.utils.to_categorical(X_train_reduced)\n",
        "print('X_train_one_hot shape:', X_train_one_hot.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5RvIN4w66Ej"
      },
      "source": [
        "Note the shape of the one-hot encoded features. For each of our 25000 training examples, we have a 20 x 1000 matrix. That is, for each of 20 token positions, we have a vector of 1000 elements containing a single 1 and 999 zeros.\n",
        "\n",
        "We can think of these 1000-dimensional one-hot arrays as **embeddings**. Each token in the input has a 1000-dimensional representation. But because of the one-hot setup, the distance between each pair of tokens is the same ([1,0,0,...], [0,1,0,...], etc.). By contrast, learned embeddings result in meaningful distances between pairs of tokens. We'll get to that soon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "296Cnt647b5c"
      },
      "source": [
        "## Logistic Regression with One-Hot Encodings\n",
        "Let's start with something familiar -- logistic regression. Since our feature representation is in 2 dimensions (20 x 1000), we need to flatten it to pass it to Keras (remember we did this with the pixel data too). Let's try two strategies for flattening.\n",
        "\n",
        "1. Flatten by *concatenating* (as we did with pixels), turning (20 x 1000) data into (20000,) data. The result is a separate feature for each token at each position.\n",
        "2. Flatten by *averaging* over token positions, turning (20 x 1000) data into (1000,) data. The result is an array with average token counts, ignoring position.\n",
        "\n",
        "NOTE: Our prior assignments have used the standard Stochastic Gradient Descent (SGD) optimizer to compute the gradient from an estimate of the loss (based on the current mini-batch). There are many alternative optimizers. Here we'll use the **Adam** optimizer, which sometimes gives better results. One key characteristic of Adam is that it effectively uses a different learning rate for each parameter rather than a fixed learning rate as in SGD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m6eebM-0dUW"
      },
      "source": [
        "# There are more optimizers aside from the SGD, like Adam. Not stochastic\n",
        "\n",
        "def build_onehot_model(average_over_positions=False):\n",
        "  \"\"\"Build a tf.keras model for one-hot data.\"\"\"\n",
        "  # Clear session and remove randomness.\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  if average_over_positions:\n",
        "    # Average.\n",
        "    # This layer averages over the first dimension of the input by default.\n",
        "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "  else:\n",
        "    # Concatenate.\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "  # Create a model\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=1,                     # output dim (for binary classification)\n",
        "      activation=\"sigmoid\"         # sigmoid activation for classification\n",
        "  ))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy',   # this is a classification task\n",
        "                optimizer='adam',             # fancy optimizer\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY3W_1-OSZ2X"
      },
      "source": [
        "Now let's try fitting the model to our training data and check performance metrics on the validation (held-out) data. But first, here's a function for plotting the learning curves given the training history object we get from Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOVmajSuMjN6"
      },
      "source": [
        "def plot_history(history):\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.xticks(range(0, len(history['loss'] + 1)))\n",
        "  plt.plot(history['loss'], label=\"training\", marker='o')\n",
        "  plt.plot(history['val_loss'], label=\"validation\", marker='o')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyE4PgX70_op"
      },
      "source": [
        "model = build_onehot_model()\n",
        "\n",
        "# Fit the model.\n",
        "history = model.fit(\n",
        "  x = X_train_one_hot,  # one-hot training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "# Convert the return value into a DataFrame so we can see the train loss \n",
        "# and binary accuracy after every epoch.\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuCh9aQPv7F_"
      },
      "source": [
        "### Comparing logistic regrerssion models \n",
        "\n",
        "Let's train the one-hot model using both the concatenating and the averaging strategies and compare the results. Let's call these *LR-C* (Logistic Regression Concatenating) and *LR-A* (Logistic Regression Averaging).\n",
        "\n",
        "### Results:\n",
        "\n",
        "NOTE:\n",
        "* Accuracy for Concatenating methods gives better results than the \n",
        "other method (Averaging) but just on the accuracy of the same data. \n",
        "* It overfits since on the validation accuracy is actually very bad.\n",
        "* In the Averaging method the training and validation scores are around the same. There is no overfitting, which I think is better.\n",
        "* My hypothesis is that if trendings results continue the accuracy for the Averaging method will get better."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenating strategy. Logistic Regresssion Concatenating LR-C\n",
        "\n",
        "model = build_onehot_model(average_over_positions=False)\n",
        "\n",
        "# Fit the model.\n",
        "history = model.fit(\n",
        "  x = X_train_one_hot,  # one-hot training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "# Convert the return value into a DataFrame so we can see the train loss \n",
        "# and binary accuracy after every epoch.\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)\n",
        "\n"
      ],
      "metadata": {
        "id": "QPDPDgbTwIQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Averaging strategy. Logistic Regression Averaging\n",
        "\n",
        "model = build_onehot_model(average_over_positions=True)\n",
        "\n",
        "# Fit the model.\n",
        "history = model.fit(\n",
        "  x = X_train_one_hot,  # one-hot training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "# Convert the return value into a DataFrame so we can see the train loss \n",
        "# and binary accuracy after every epoch.\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "4cIVwGR9220k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJIBRqK7lsjG"
      },
      "source": [
        "## Logistic Regression with Embeddings\n",
        "Next, let's train model that replaces one-hot representations of each token with learned embeddings.\n",
        "\n",
        "The code below uses a Keras Embedding layer, which expects to receive a sparse (rather than one-hot) representation. That is, it expects a (padded) sequence of token ids; for each id, it looks up the corresponding embedding vector.\n",
        "\n",
        "\n",
        "### Note\n",
        "Instead of logistic regression with One-Hot-Encodings. Let's do Embeddings!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho6uOeCaBs2e"
      },
      "source": [
        "def build_embeddings_model(average_over_positions=False,\n",
        "                           vocab_size=1000,\n",
        "                           sequence_length=20,\n",
        "                           embedding_dim=2):\n",
        "  \"\"\"Build a tf.keras model using embeddings.\"\"\"\n",
        "  # Clear session and remove randomness.\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  emb_layer = tf.keras.layers.Embedding(\n",
        "      input_dim=vocab_size, # Vocab size 1000. Number of possible words\n",
        "      output_dim=embedding_dim, # Number of dimensions for each word\n",
        "      input_length=sequence_length)\n",
        "\n",
        "  model.add(\n",
        "      emb_layer\n",
        "  )\n",
        "\n",
        "  if average_over_positions:\n",
        "    # Average.\n",
        "    # This layer averages over the first dimension of the input by default.\n",
        "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "  else:\n",
        "    # Concatenate.\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=1,                     # output dim (for binary classification)\n",
        "      activation='sigmoid'         # apply the sigmoid function!\n",
        "  ))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', \n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyhoEjAiFSNB"
      },
      "source": [
        "Try training the model as before. We'll use the averaging strategy rather than the concatenating strategy for dealing with the token sequence. That is, we'll look up embedding vectors for each token. Then we'll average them to produce a single vector. Then we'll traing a logistic regression with that vector as input to predict the binary label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYUE5UwkxoU8"
      },
      "source": [
        "model = build_embeddings_model(average_over_positions=True,\n",
        "                               vocab_size=1000,\n",
        "                               sequence_length=20,\n",
        "                               embedding_dim=2)\n",
        "history = model.fit(\n",
        "  x = X_train_reduced,  # our sparse padded training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3k__61hFnag"
      },
      "source": [
        "### Experiments with embeddings \n",
        "Train 3 models with embedding sizes in [4,16,64], keeping other settings fixed using the averaging strategy. Repeat using the concatenating strategy. \n",
        "\n",
        "### Results\n",
        "This are the results for the same model and Number of epochs as 5\n",
        "* Embedding size is 2: Accuracy 71.9%. Validation Accuracy 71%. Basically the same accuracy for validation and training data. Model generalizing brilliantlly.\n",
        "* Embedding size is 4: Accuracy 73.02%. Validation Accuracy 71.64%. Model generalizing very well but the difference between training and validation accuracies are increasing. Validation accuracy went up by 0.64%\n",
        "* Embedding size is 16: Accuracy 74.84%. Validation Accuracy 72.68%. Model generalizing very well but the difference between training and validation accuracies are increasing. Validation accuracy increasing just a bit.\n",
        "* Embedding size is 64: Accuracy 75.51%. Validation Accuracy 72.84%. Accuracy 74.84%. Validation Accuracy 72.68%."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# YOUR CODE HERE\n",
        "embedding_sizes = [4, 16, 64]\n",
        "\n",
        "for embed_size in embedding_sizes:\n",
        "  print('Embedding Dimension Size, ', embed_size)\n",
        "  model = build_embeddings_model(average_over_positions=True,\n",
        "                                vocab_size=1000,\n",
        "                                sequence_length=20,\n",
        "                                embedding_dim=embed_size)\n",
        "  history = model.fit(\n",
        "    x = X_train_reduced,  # our sparse padded training data\n",
        "    y = Y_train,          # corresponding binary labels\n",
        "    epochs=5,             # number of passes through the training data\n",
        "    batch_size=64,        # mini-batch size\n",
        "    validation_split=0.1, # use a fraction of the examples for validation\n",
        "    verbose=1             # display some progress output during training\n",
        "    )\n",
        "\n",
        "  history = pd.DataFrame(history.history)\n",
        "  plot_history(history)"
      ],
      "metadata": {
        "id": "AIlNj3jfxKbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2dWOuxqKHA6"
      },
      "source": [
        "## Inspecting Learned Embeddings\n",
        "Let's retrieve the learned embedding parameters from the trained model and plot the token embeddings.\n",
        "\n",
        "The model layers in a Keras Sequential model are stored as a list and the embeddings are the first layer. We can use the get_weights() function to get a numpy array with the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfsbGSwkaFjo"
      },
      "source": [
        "# Display the model layers.\n",
        "display(model.layers)\n",
        "\n",
        "# Retrieve the embeddings layer, which itself is wrapped in a list.\n",
        "embeddings = model.layers[0].get_weights()[0]\n",
        "display(embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apPWscNwcXTE"
      },
      "source": [
        "Now we'll use a fancy plotting tool called *plotly* to show the embeddings with hovertext so you can move your mouse over the points to see the corresponding tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RZMTrA0KttL"
      },
      "source": [
        "def plot_2d_embeddings(embeddings, id_start=1, count=100):\n",
        "  # Get 1st and 2nd embedding dims for the desired tokens.\n",
        "  x1 = embeddings[id_start:id_start+count, 0]\n",
        "  x2 = embeddings[id_start:id_start+count, 1]\n",
        "  \n",
        "  # Get the corresponding words from the reverse index (for labeling).\n",
        "  tokens = [reverse_index[i] for i in range(id_start, id_start+count)]\n",
        "\n",
        "  # Plot with the plotly library.\n",
        "  data = plotly.Scatter(x=x1, y=x2, text=tokens,\n",
        "                        mode='markers', textposition='bottom left',\n",
        "                        hoverinfo='text')\n",
        "  fig = plotly.Figure(data=[data],\n",
        "                      layout=plotly.Layout(title=\"Word Embeddings\",\n",
        "                                           hovermode='closest'))\n",
        "  fig.show()\n",
        "\n",
        "# Very frequent tokens tend to be more syntactic than semantic, so let's plot\n",
        "# some rarer words.    \n",
        "plot_2d_embeddings(embeddings, id_start=500, count=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GHwKs-6uyxMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Pre-trained Embeddings\n",
        "\n",
        "Let's see if we can improve performance using pre-trained embeddings. "
      ],
      "metadata": {
        "id": "IJQG8th4yVvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "14-zKg-9y5Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The file we just downloaded contains pre-trained GloVe embeddings. If you open the folder, you'll see that embeddings of different sizes are provided (50D, 100D, 200D, 300D). Let's start with the 50D ones.\n"
      ],
      "metadata": {
        "id": "MqpBckzYzM8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "embd_path = \"./glove.6B.50d.txt\" # 50 dimension embedding size\n",
        "\n",
        "word_to_embd = {}\n",
        "with open(embd_path) as f:\n",
        "  for line in f:\n",
        "    word, embd = line.split(maxsplit=1)\n",
        "    embd = np.fromstring(embd, \"f\", sep=\" \") # '1 2' -> [1, 2]\n",
        "    word_to_embd[word] = embd\n",
        "\n",
        "print(f'Total number of embeddings: {len(word_to_embd)}')"
      ],
      "metadata": {
        "id": "Z44ZQ6Wh1APE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Embedding\n",
        "word_to_embd['movie'] # Note this really cool embeddings. The dictionary's key is the word and the value are the embeddings (numeric values)"
      ],
      "metadata": {
        "id": "mkFuH9ZLr6dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_x_vocab(X_train):\n",
        "  vocab = set()\n",
        "\n",
        "  for i in range(len(X_train)):\n",
        "    token_ids = X_train[i]\n",
        "    for token in token_ids:\n",
        "      token = reverse_index.get(token, \"#\")\n",
        "      vocab.add(token)\n",
        "\n",
        "  return vocab\n",
        "\n"
      ],
      "metadata": {
        "id": "wGHWdb1M3kz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 50\n",
        "hits = 0\n",
        "misses = 0\n",
        "vocab = get_x_vocab(X_train)\n",
        "num_tokens = len(vocab) + 2\n",
        "\n",
        "# Prepare embedding matrix\n",
        "new_embed_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for i, word in enumerate(vocab):\n",
        "    embedding_vector = word_to_embd.get(word,np.zeros(embedding_dim))\n",
        "    new_embed_matrix[i] = embedding_vector\n",
        "\n",
        "model.layers[0].weights[0] = new_embed_matrix"
      ],
      "metadata": {
        "id": "do6VBxVY2Zz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Excercise\n",
        "\n",
        "Repeat our experiments where we trained 6 models, but use the pre-trained embedding matrix from Glove."
      ],
      "metadata": {
        "id": "EDWN21OyEDCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Your code goes here\n",
        "\n",
        "# Pre-trained embedding matrix \n",
        "\n",
        "history = model.fit(\n",
        "  x = X_train_reduced,  # our sparse padded training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)\n"
      ],
      "metadata": {
        "id": "CyDOqKmDENS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "List what you learn (at least 3 bullet points):"
      ],
      "metadata": {
        "id": "ioJg5y9AEPCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* I noticed that with the new embedding matrix the results for accuracy and validation accuracy went up.\n",
        "* The validation and training accuracies seem to differ a bit more thatn previous models. So maybe this type of models tend to not generalize as well.\n",
        "* Embeddings perform better than one-hot-encodings, the accuracy in both training and validation data is better\n",
        "* One-hot-encoding are using a lot of memory space. Embeddings of small size embeddings are better."
      ],
      "metadata": {
        "id": "diCBFN7o5aHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Instructions\n",
        "\n",
        "1. File > Download .ipynb\n",
        "2. Go to Blackboard, find the submission page, and upload the .ipynb file you just downloaded."
      ],
      "metadata": {
        "id": "Tn_nLbelRkNS"
      }
    }
  ]
}
